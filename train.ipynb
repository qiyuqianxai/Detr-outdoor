{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入必须的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入模块\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pycocotools in /data/nas/workspace/envs/python3.6/site-packages (2.0.4)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.6/site-packages (from pycocotools) (3.3.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pycocotools) (1.19.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "模块导入成功！\n"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "print(\"导入模块\")\n",
    "!pip install pycocotools\n",
    "# !pip uninstall torchvision\n",
    "# !pip install -U torchvision==0.8.2\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model\n",
    "import time\n",
    "print(\"模块导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高级参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "定义高级参数函数\n",
      "定义完成\n"
     ]
    }
   ],
   "source": [
    "print(\"定义高级参数函数\")\n",
    "def get_args_parser():\n",
    "\n",
    "    custom_param = config()\n",
    "\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=custom_param.lr, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=custom_param.batch_size, type=int)\n",
    "    parser.add_argument('--weight_decay', default=custom_param.weight_decay, type=float)\n",
    "    parser.add_argument('--epochs', default=custom_param.epochs, type=int)\n",
    "    parser.add_argument('--one_epoch_steps', default=custom_param.one_epoch_steps, type=int)\n",
    "    parser.add_argument('--lr_drop', default=custom_param.lr_drop, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=1, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=1, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=64, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=4, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=4, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--num_classes', default=custom_param.num_classes, type=int,\n",
    "                        help='#classes in your dataset, which can override the value hard-coded in file models/detr.py')\n",
    "    parser.add_argument('--dataset_file', default=custom_param.dataset_file)\n",
    "    parser.add_argument('--coco_path', default=custom_param.coco_path, type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default=custom_param.output_dir,\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default=custom_param.device,\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=custom_param.seed, type=int)\n",
    "    # parser.add_argument('--resume', default=custom_param.resume, help='resume from checkpoint')\n",
    "    parser.add_argument('--resume', action='store_true')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=custom_param.num_workers, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=3, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    return parser\n",
    "print(\"定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  基本参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置基本参数\n",
      "设置完成\n"
     ]
    }
   ],
   "source": [
    "print(\"设置基本参数\")\n",
    "class config:\n",
    "    lr = 1e-4 # 学习率\n",
    "    lr_backbone = 1e-5 # backbone的学习率\n",
    "    lr_drop = 50 # 学习率衰减\n",
    "    batch_size = 1 # 批次大小\n",
    "    weight_decay = 1e-4 # 权重衰减\n",
    "    epochs = 1 # 训练回合数\n",
    "    one_epoch_steps = 2 # 一个epoch中的迭代次数\n",
    "    # dataset parameters\n",
    "    num_classes = 5 # 类别数\n",
    "\n",
    "    dataset_file = \"custom\" # 使用自定义的数据库函数，不需要更改\n",
    "\n",
    "    coco_path = r\"MyDatasets\" # 自定义数据集路径的路径\n",
    "\n",
    "    # device = 'cuda' if torch.cuda.is_available() else \"cpu\" # device to use for training / testing\n",
    "\n",
    "    device = \"cpu\"\n",
    "\n",
    "    seed = 42 # 随机种子\n",
    "\n",
    "    resume = \"detr-r50_no-class-head.pth\" # 'resume from checkpoint'\n",
    "\n",
    "    num_workers = 1 # 读取数据的进程数\n",
    "\n",
    "    output_dir = \"outputs\" # 保存模型的路径\n",
    "    \n",
    "print(\"设置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练前的准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载参数\n",
      "Not using distributed mode\n",
      "Namespace(aux_loss=True, backbone='resnet50', batch_size=1, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='MyDatasets', dataset_file='custom', dec_layers=1, device='cpu', dice_loss_coef=1, dilation=False, dim_feedforward=64, dist_url='env://', distributed=False, dropout=0.1, enc_layers=1, eos_coef=0.1, epochs=1, eval=False, frozen_weights=None, giou_loss_coef=2, hidden_dim=4, lr=0.0001, lr_backbone=1e-05, lr_drop=50, mask_loss_coef=1, masks=False, nheads=4, num_classes=5, num_queries=100, num_workers=1, one_epoch_steps=2, output_dir='outputs', position_embedding='sine', pre_norm=False, remove_difficult=False, resume=False, seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=3)\n",
      "加载完成\n"
     ]
    }
   ],
   "source": [
    "# 获取配置参数\n",
    "print(\"加载参数\")\n",
    "args = get_args_parser().parse_args([])\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 初始化分布式模型参数配置\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "# 打印参数配置信息\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# 固定随机种子以便于复现\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "print(\"加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建立模型\n",
      "DETR(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=4, out_features=4, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=4, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=64, out_features=4, bias=True)\n",
      "          (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=4, out_features=4, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=4, out_features=4, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=4, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=64, out_features=4, bias=True)\n",
      "          (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (class_embed): Linear(in_features=4, out_features=6, bias=True)\n",
      "  (bbox_embed): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=4, out_features=4, bias=True)\n",
      "      (1): Linear(in_features=4, out_features=4, bias=True)\n",
      "      (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (query_embed): Embedding(100, 4)\n",
      "  (input_proj): Conv2d(2048, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (backbone): Joiner(\n",
      "    (0): Backbone(\n",
      "      (body): IntermediateLayerGetter(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d()\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      ")\n",
      "模型建立完成\n"
     ]
    }
   ],
   "source": [
    "print(\"建立模型\")\n",
    "!cp /home/admin/jupyter/resnet50-19c8e357.pth /home/admin/.cache/torch/hub/checkpoints/\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "# 将模型分为多卡分布式模型和单卡训练模型，根据需求使用模型\n",
    "model_without_ddp = model\n",
    "if args.distributed:\n",
    "    # 使用分布式模型\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    model_without_ddp = model.module\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "print(\"模型建立完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置训练参数配置\n",
      "训练参数量: 8686\n",
      "配置完成\n"
     ]
    }
   ],
   "source": [
    "print(\"设置训练参数配置\")\n",
    "# 冻结骨干网络\n",
    "for n, p in model.named_parameters():\n",
    "    if (\"backbone\" in n or \"transformer\" in n) and p.requires_grad:\n",
    "        p.requires_grad = False\n",
    "# 获取并打印模型参数数量\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('训练参数量:', n_parameters)\n",
    "\n",
    "# 给backbone和主网络分别配置优化器\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "]\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                              weight_decay=args.weight_decay)\n",
    "\n",
    "# 设置学习率衰减策略\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "print(\"配置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置数据参数\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "设置完成\n"
     ]
    }
   ],
   "source": [
    "print(\"设置数据参数\")\n",
    "# 读取训练集，验证集\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "# 根据是否采用分布式训练分配每一batch的数据\n",
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    # 随机取样\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "# 设置训练dataloader\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "# 设置测试dataloader\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    # 对验证数据做单独处理，一般是不对验证集做数据增强\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "# 如果冻结参数，则读取预训练权重进行训练\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# 保存路径\n",
    "output_dir = Path(args.output_dir)\n",
    "print(\"设置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取预训练配置\n",
      "读取完成\n"
     ]
    }
   ],
   "source": [
    "print(\"读取预训练配置\")\n",
    "if args.resume:\n",
    "    # 读取预训练模型配置，包括权重，学习率，epoch等参数，也可以读取之前自己未训练完成的模型继续训练\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    # 读取模型权重\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    # 读取训练配置参数\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "print(\"读取完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:1/1\n",
      "step:1/2 || Total Loss: 6.8409 || 88.0018s/step\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练\")\n",
    "# 记录训练时间\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    print('\\n' + '-' * 10 + 'Train one epoch.' + '-' * 10)\n",
    "    print('Epoch:'+ str(epoch+1) + '/' + str(args.epochs))\n",
    "    # 训练一个epoch\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm,args.one_epoch_steps)\n",
    "    # 学习率衰减\n",
    "    lr_scheduler.step()\n",
    "    # 保存模型\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "# 显示训练总耗时\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
